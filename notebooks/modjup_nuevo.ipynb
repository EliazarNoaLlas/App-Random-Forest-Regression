{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LIBRERIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CARGANDO EL CONJUNTO DE DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"../Data/Melsol-test.xlsx\")\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EDA (Analisis Exploratorio de datos)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualización de la distribución de variables\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, column in enumerate(df_train.columns):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.histplot(df_train[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VERIFICAR DATOS ATIPICOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar el estilo de los gráficos\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Seleccionar las columnas numéricas\n",
    "numeric_columns = df_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Crear gráficos de caja para identificar valores atípicos\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.boxplot(x=df_train[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "* Columna PRODUCTOS ALMACENADOS\n",
    "* Columna DEMNADA DEL PRODUCTO\n",
    "* Columna PRODUCTOS VENDIDOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(column, cap_value=None):\n",
    "    # Calcula el rango intercuartílico (IQR)\n",
    "    Q1 = column.quantile(0.25)\n",
    "    Q3 = column.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define los límites superior e inferior para identificar los valores atípicos\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Acotar los valores atípicos\n",
    "    if cap_value is not None:\n",
    "        column = column.clip(lower=lower_limit, upper=cap_value)\n",
    "    else:\n",
    "        column = column.clip(lower=lower_limit, upper=upper_limit)\n",
    "    \n",
    "    return column\n",
    "\n",
    "# Aplicar la función a las columnas con valores atípicos\n",
    "df_train['PRODUCTOS ALMACENADOS'] = handle_outliers(df_train['PRODUCTOS ALMACENADOS'])\n",
    "df_train['DEMANDA DEL PRODUCTO'] = handle_outliers(df_train['DEMANDA DEL PRODUCTO'])\n",
    "df_train['PRODUCTOS VENDIDOS'] = handle_outliers(df_train['PRODUCTOS VENDIDOS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "correlation_matrix = df_train.corr()\n",
    "\n",
    "# Crear una máscara para la parte triangular inferior\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Configurar el estilo de los gráficos\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Crear la figura y el eje (axis)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Crear el mapa de calor con la matriz de correlación\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# Configurar el título\n",
    "plt.title('Correlation Matrix - Upper Triangle Only')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVACIONES** \n",
    "*CORRELACION FUERTE POSITIVA*\n",
    "* PRODUCTOS ALMACENADOS y DEMANDA DEL PRODUCTO: 0.838341\n",
    "* PRODUCTOS ALMACENADOS y PRODUCTOS VENDIDOS: 0.848399\n",
    "* DEMANDA DEL PRODUCTO y PRODUCTOS VENDIDOS: 0.978426\n",
    "* Correlación Fuerte Negativa:\n",
    "\n",
    "*CORRELACION FUERTE NEGATIVA*\n",
    "\n",
    "* FESTIVIDAD y PRODUCTOS VENDIDOS: -0.237186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seleccionar solo la columna de correlaciones con \"PRODUCTOS VENDIDOS\"\n",
    "correlations_with_target = correlation_matrix['PRODUCTOS VENDIDOS'].drop('PRODUCTOS VENDIDOS')\n",
    "\n",
    "# Crear un gráfico de barras para visualizar las correlaciones\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations_with_target.sort_values().plot(kind='barh', color='skyblue')\n",
    "plt.title('Correlación con \"PRODUCTOS VENDIDOS\"')\n",
    "plt.xlabel('Correlación')\n",
    "plt.ylabel('Variable')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREPROCESAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el número de valores únicos de cada columna\n",
    "for column in df_train.columns:\n",
    "    num_unique_values = df_train[column].nunique()\n",
    "    print(f'Columna: {column}, Número de Valores Únicos: {num_unique_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analizar_y_eliminar_ruido(df):\n",
    "    \"\"\"\n",
    "    Analiza el número de valores únicos en cada columna y decide si eliminar la columna si la cantidad de valores únicos es igual a 1.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame de pandas\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame modificado sin las columnas identificadas como ruido.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar una lista para almacenar las columnas a eliminar\n",
    "    columnas_a_eliminar = []\n",
    "\n",
    "    # Iterar sobre cada columna del DataFrame\n",
    "    for columna in df.columns:\n",
    "        # Verificar si la cantidad de valores únicos es igual a 1\n",
    "        if df[columna].nunique() == 1:\n",
    "            columnas_a_eliminar.append(columna)\n",
    "            print(f'Columna \"{columna}\" tiene un solo valor único. Se considera ruido.')\n",
    "\n",
    "    # Eliminar las columnas identificadas como ruido\n",
    "    df_sin_ruido = df.drop(columnas_a_eliminar, axis=1)\n",
    "\n",
    "    print(f'\\nColumnas eliminadas: {columnas_a_eliminar}')\n",
    "\n",
    "    return df_sin_ruido\n",
    "\n",
    "# Ejemplo de uso\n",
    "df_sin_ruido = analizar_y_eliminar_ruido(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin_ruido.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin_ruido.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin_ruido.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las características (X) y la variable objetivo (y)\n",
    "X = df_sin_ruido.drop('PRODUCTOS VENDIDOS', axis=1)\n",
    "y = df_sin_ruido['PRODUCTOS VENDIDOS']\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Imprimir las formas de los conjuntos resultantes\n",
    "print(\"Forma de X_train:\", X_train.shape)\n",
    "print(\"Forma de X_test:\", X_test.shape)\n",
    "print(\"Forma de y_train:\", y_train.shape)\n",
    "print(\"Forma de y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Crear el modelo de Random Forest Regression\n",
    "bosque = RandomForestRegressor(n_estimators=100,\n",
    "                                criterion=\"squared_error\",\n",
    "                                max_features=\"sqrt\",\n",
    "                                bootstrap=True,\n",
    "                                oob_score=True,\n",
    "                                random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "bosque.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Predecir en un nuevo conjunto de datos\n",
    "nuevos_datos = [[1, 10, 0.4, 5.0, 1, 0]]  # Ajusta estos valores según tus datos\n",
    "prediccion = bosque.predict(nuevos_datos)\n",
    "print(\"Predicción:\", prediccion)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "predicciones_test = bosque.predict(X_test.values)\n",
    "mse = mean_squared_error(y_test.values, predicciones_test)\n",
    "print(\"Error cuadrático medio en el conjunto de prueba:\", mse)\n",
    "\n",
    "# Imprimir la puntuación R^2 en el conjunto de entrenamiento\n",
    "print(\"Puntuación R^2 en el conjunto de entrenamiento:\", bosque.score(X_train.values, y_train.values))\n",
    "\n",
    "# Imprimir la puntuación R^2 en el conjunto de prueba\n",
    "print(\"Puntuación R^2 en el conjunto de prueba:\", bosque.score(X_test.values, y_test.values))\n",
    "\n",
    "# Imprimir la puntuación \"out-of-bag\" (OOB)\n",
    "print(\"Puntuación OOB:\", bosque.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Crear el modelo de Random Forest Regression\n",
    "bosque = RandomForestRegressor(n_estimators=100,\n",
    "                                criterion=\"squared_error\",  # Utilizar \"squared_error\" en lugar de \"mse\"\n",
    "                                max_features=\"sqrt\",\n",
    "                                bootstrap=True,\n",
    "                                oob_score=True,\n",
    "                                random_state=42)\n",
    "\n",
    "# Definir la métrica a utilizar (en este caso, negativo del Error Cuadrático Medio para que sea coherente con la validación cruzada)\n",
    "metrica = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Realizar validación cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Puedes ajustar el número de divisiones (folds)\n",
    "resultados_cross_val = cross_val_score(bosque, X.values, y.values, cv=kf, scoring=metrica)\n",
    "\n",
    "# Imprimir los resultados de la validación cruzada\n",
    "print(\"Resultados de la validación cruzada:\")\n",
    "print(\"MSE por fold:\", resultados_cross_val)\n",
    "print(\"Promedio MSE:\", resultados_cross_val.mean())\n",
    "\n",
    "# Entrenar el modelo en todo el conjunto de datos\n",
    "bosque.fit(X.values, y.values)\n",
    "\n",
    "# Predecir en un nuevo conjunto de datos\n",
    "nuevos_datos = [[1, 10, 0.4, 5.0, 1,0]]  # Ajusta estos valores según tus datos\n",
    "prediccion = bosque.predict(nuevos_datos)\n",
    "print(\"Predicción:\", prediccion)\n",
    "\n",
    "# Imprimir la puntuación R^2 en el conjunto completo\n",
    "print(\"Puntuación R^2 en el conjunto completo:\", bosque.score(X.values, y.values))\n",
    "\n",
    "# Imprimir la puntuación \"out-of-bag\" (OOB)\n",
    "print(\"Puntuación OOB:\", bosque.oob_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Suponiendo que bosque es tu modelo entrenado\n",
    "for i, arbol in enumerate(bosque.estimators_[:3]):  # Limitar a los primeros tres árboles\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    tree.plot_tree(arbol, feature_names=X.columns.tolist(), filled=True, rounded=True)\n",
    "    plt.title(f\"Árbol {i+1}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_excel(\"Melsol-test.xlsx\")\n",
    "\n",
    "x = df.drop([\"PRODUCTOS VENDIDOS\"], axis=1)\n",
    "y = df[\"PRODUCTOS VENDIDOS\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.40, random_state=42)\n",
    "#sc = StandardScaler()\n",
    "#x_train = sc.fit_transform(x_train)\n",
    "#x_test = sc.transform(x_test)\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=300, max_depth=10)\n",
    "regressor = model.fit(x_train, y_train)\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print (\"Mean Absolute Error:\" , metrics.mean_absolute_error(y_test, y_pred))\n",
    "print (\"Mean Squared Error:\" , metrics.mean_squared_error(y_test, y_pred))\n",
    "print (\"Root Mean Squared Error:\" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(f\"R²: {r2}\")\n",
    "\n",
    "# Gráfico de Dispersión (Scatter Plot)\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.scatter (y_test, y_pred, color='red', label='Comparison of Prediction between Actual & Prediction data')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Gráfico de Dispersión')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Data Actual')\n",
    "plt.show()\n",
    "\n",
    "# Curva de regresión\n",
    "x_range = np.linspace(min(y_test), max(y_test), 100)\n",
    "y_range = x_range  # Línea de regresión ideal (y = x)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot(x_range, y_range, color=\"red\", linestyle=\"--\", label=\"Línea de Regresión Ideal\")\n",
    "plt.xlabel(\"Predicciones\")\n",
    "plt.ylabel(\"Data Actual\")\n",
    "plt.title(\"Curva de Regresión\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ventas_reales = pd.read_excel(\"Diclofenaco-prediccion.xlsx\")\n",
    "\n",
    "predicciones_nuevas = model.predict(ventas_reales)\n",
    "\n",
    "print(f\"Predicción de ventas para el siguiente mes 1: {predicciones_nuevas[0]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 2: {predicciones_nuevas[1]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 3: {predicciones_nuevas[2]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 4: {predicciones_nuevas[3]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 5: {predicciones_nuevas[4]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 6: {predicciones_nuevas[5]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 7: {predicciones_nuevas[6]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 8: {predicciones_nuevas[7]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 9: {predicciones_nuevas[8]}\")\n",
    "print(f\"Predicción de ventas para el siguiente mes 10: {predicciones_nuevas[9]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"../Data/Data Set - Prueba-Test.xlsx\", engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
